# OpenAI Configuration
# Required: Your OpenAI API key for GPT-5 access
OPENAI_API_KEY=your_openai_api_key_here

# Optional: GPT-5 model to use (default: gpt-5-mini)
# Options: gpt-5, gpt-5-mini, gpt-5-nano, gpt-5-codex
# - gpt-5: Best for complex reasoning, broad world knowledge, and multi-step agentic tasks
# - gpt-5-mini: Cost-optimized reasoning and chat; balances speed, cost, and capability (default)
# - gpt-5-nano: High-throughput tasks, especially simple instruction-following or classification
# - gpt-5-codex: Optimized for code generation, refactoring, debugging, and code explanations
OPENAI_MODEL=gpt-5-mini

# Optional: Reasoning effort level (default: medium)
# Options: minimal, low, medium, high
# - minimal: Very few reasoning tokens, fastest time-to-first-token
# - low: Favors speed and fewer tokens
# - medium: Balanced reasoning and speed (default)
# - high: More thorough reasoning, best for complex tasks
OPENAI_REASONING_EFFORT=medium

# Optional: Output verbosity level (default: medium)
# Options: low, medium, high
# - low: Concise answers, shorter code with minimal commentary
# - medium: Balanced output length
# - high: Thorough explanations, longer structured code with inline explanations
OPENAI_VERBOSITY=medium

# Optional: Maximum tokens for output (default: 4096)
OPENAI_MAX_TOKENS=4096

# Optional: Maximum retries for network errors (default: 3)
# Range: 0-5. Set to 0 to disable retries
OPENAI_MAX_RETRIES=3

# Optional: Base delay for retries in milliseconds (default: 1000)
# Uses exponential backoff: delay * 2^attempt
OPENAI_RETRY_DELAY=1000

# Server Configuration
# Optional: Server name (default: kortx-mcp)
SERVER_NAME=kortx-mcp

# Optional: Server version (default: 0.1.0)
SERVER_VERSION=0.1.0

# Optional: Port for HTTP transport (default: 3000)
# Note: Currently only stdio transport is implemented
PORT=3000

# Optional: Transport mode (default: stdio)
# Options: stdio, streaming
TRANSPORT=stdio

# Logging Configuration
# Optional: Log level for structured logging (default: info)
# Options: debug, info, warn, error
# - debug: Detailed information for debugging (includes LLM requests, context gathering)
# - info: General informational messages (tool calls, responses, lifecycle events)
# - warn: Warning messages for potentially problematic situations
# - error: Error messages for failures
LOG_LEVEL=info

# Optional: Enable audit logging to .audit/kortx-mcp.log (default: false)
# When enabled with stdio transport, logs are written to .audit/kortx-mcp.log
# When disabled, logs go to stdout/stderr (when not using stdio transport)
# The .audit directory is automatically created if it doesn't exist
AUDIT_LOGGING=false

# Optional: Node environment (affects log formatting)
# - development: Pretty-printed, colorized logs with timestamps
# - production: JSON-formatted logs for log aggregation systems
NODE_ENV=development

# Context Gathering Configuration
# Optional: Enable Serena MCP integration (default: true)
ENABLE_SERENA=true

# Optional: Enable Memory MCP integration (default: true)
ENABLE_MEMORY=true

# Optional: Enable CCLSP MCP integration (default: true)
ENABLE_CCLSP=true

# Optional: Maximum tokens for context gathering (default: 32000)
MAX_CONTEXT_TOKENS=32000

# Optional: Include file content in context (default: true)
INCLUDE_FILE_CONTENT=true

# Optional: Include git history in context (default: false)
INCLUDE_GIT_HISTORY=false

# Security Configuration
# Optional: Enable rate limiting (default: true)
ENABLE_RATE_LIMITING=true

# Optional: Maximum requests per hour per client (default: 100)
MAX_REQUESTS_PER_HOUR=100

# Optional: Maximum tokens per single request (default: 50000)
MAX_TOKENS_PER_REQUEST=50000

# Optional: Maximum tokens per hour per client (default: 500000)
MAX_TOKENS_PER_HOUR=500000

# Optional: Request timeout in milliseconds (default: 60000)
REQUEST_TIMEOUT_MS=60000

# Optional: Maximum input size in bytes (default: 100000 - 100KB)
MAX_INPUT_SIZE=100000

# Perplexity Configuration
# Required: Your Perplexity API key for Sonar model access
PERPLEXITY_API_KEY=your_perplexity_api_key_here

# Optional: Perplexity Sonar model to use (default: sonar)
# Options: sonar, sonar-pro, sonar-deep-research, sonar-reasoning, sonar-reasoning-pro
# - sonar: Lightweight search model with fast response times (default)
# - sonar-pro: Advanced search with enhanced accuracy and comprehensive results
# - sonar-deep-research: Exhaustive research with in-depth analysis and extensive sourcing
# - sonar-reasoning: Fast reasoning model optimized for analytical tasks
# - sonar-reasoning-pro: Premier reasoning model for complex analytical challenges
PERPLEXITY_MODEL=sonar

# Optional: Temperature for response generation (default: 0.2)
# Range: 0-2. Lower values make output more focused and deterministic
PERPLEXITY_TEMPERATURE=0.2

# Optional: Maximum tokens for output (default: 4096)
PERPLEXITY_MAX_TOKENS=4096

# Optional: Search mode (default: web)
# Options: academic, sec, web
# - academic: Search academic papers and scholarly content
# - sec: Search SEC filings and financial documents
# - web: General web search across all domains
PERPLEXITY_SEARCH_MODE=web

# Optional: Return images in search results (default: false)
PERPLEXITY_RETURN_IMAGES=false

# Optional: Return related questions for follow-up (default: false)
PERPLEXITY_RETURN_RELATED_QUESTIONS=false

# GPT Image Configuration
# Optional: Image model to use (default: gpt-image-1)
# Currently only 'gpt-image-1' is supported
GPT_IMAGE_MODEL=gpt-image-1

# Optional: Default image dimensions (default: auto)
# Options: 1024x1024 (square), 1536x1024 (landscape), 1024x1536 (portrait), auto
# - 1024x1024: Square format, balanced composition
# - 1536x1024: Landscape format, wider scenes and panoramas
# - 1024x1536: Portrait format, vertical subjects and compositions
# - auto: Model chooses optimal size based on prompt (default)
GPT_IMAGE_SIZE=auto

# Optional: Default rendering quality (default: auto)
# Options: low, medium, high, auto
# - low: Fastest generation, 272-408 tokens, suitable for drafts
# - medium: Balanced quality, 1056-1584 tokens, good for most uses
# - high: Best quality, 4160-6240 tokens, detailed final outputs
# - auto: Model chooses quality based on prompt complexity (default)
GPT_IMAGE_QUALITY=auto

# Optional: Default background transparency (default: auto)
# Options: transparent, opaque, auto
# - transparent: Generate with transparent background (PNG/WebP only)
# - opaque: Generate with opaque background
# - auto: Model chooses based on prompt context (default)
GPT_IMAGE_BACKGROUND=auto

# Optional: Default output image format (default: png)
# Options: png, jpeg, webp
# - png: Lossless format, supports transparency, larger file size
# - jpeg: Lossy format, smaller file size, no transparency support
# - webp: Modern format with compression, supports transparency
GPT_IMAGE_FORMAT=png

# Optional: Default compression level for JPEG/WebP (default: 85)
# Range: 0-100. Higher values = better quality but larger file size
# Only applies to jpeg and webp formats (ignored for png)
GPT_IMAGE_COMPRESSION=85

# Optional: Default input fidelity for preserving input image details (default: low)
# Options: low, high
# - low: Standard preservation for general editing
# - high: Better preserve details like faces, logos, textures when editing images
GPT_IMAGE_FIDELITY=low

# Optional: Maximum number of images per request (default: 4)
# Range: 1-10. Controls how many images can be generated in a single request
GPT_IMAGE_MAX_IMAGES=4

# Optional: Default partial images for streaming (default: 0)
# Options: 0, 1, 2, 3
# - 0: Only return final image, no progressive updates
# - 1-3: Return 1-3 partial images during generation for progress feedback
# Higher values provide more frequent updates but consume more tokens
GPT_IMAGE_PARTIAL_IMAGES=0

# Optional: API timeout in milliseconds (default: 120000 - 2 minutes)
# Image generation can take longer than text generation, especially for high quality
# Recommended settings based on quality:
# - low quality: 60000ms (1 minute)
# - medium quality: 120000ms (2 minutes) - default
# - high quality: 180000ms (3 minutes)
# - multiple images or editing: 240000ms (4 minutes)
# If you see timeout errors, try increasing this value
GPT_IMAGE_TIMEOUT=120000

# Optional: Enable cost tracking for image generation (default: true)
# Tracks token usage and estimated costs for image operations
GPT_IMAGE_ENABLE_COST_TRACKING=true

# Optional: Cost alert threshold in dollars (default: 10.0)
# Warning logged when cumulative image generation costs exceed this threshold
GPT_IMAGE_COST_ALERT_THRESHOLD=10.0
