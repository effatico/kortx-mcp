# MCP Consultant - Environment Configuration

# ============================================
# OpenAI Configuration
# ============================================

# OpenAI API Key (REQUIRED)
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your-openai-api-key-here

# Default OpenAI Model
# Options: gpt-5, gpt-5-mini, gpt-5-nano
# Default: gpt-5
OPENAI_MODEL=gpt-5

# Reasoning Effort Level
# Options: minimal, low, medium, high
# - minimal: Fast, deterministic tasks (extraction, formatting)
# - low: Faster responses with less computation
# - medium: Balanced performance (default)
# - high: Maximum reasoning depth
# Note: gpt-5-pro only supports 'high', gpt-5-codex doesn't support 'minimal'
# Default: medium
OPENAI_REASONING_EFFORT=medium

# Maximum Tokens per Request
# Controls the maximum length of model responses
# Default: 4096
OPENAI_MAX_TOKENS=4096

# Temperature (0-2)
# Controls randomness in responses
# - 0: Deterministic, focused responses
# - 1: Balanced creativity and consistency
# - 2: Maximum creativity and randomness
# Default: 0.7
OPENAI_TEMPERATURE=0.7

# ============================================
# MCP Server Configuration
# ============================================

# Server Name
# Default: llm-consultant-mcp
SERVER_NAME=llm-consultant-mcp

# Server Version
# Default: 0.1.0
SERVER_VERSION=0.1.0

# Server Port (for HTTP transport)
# Only used when TRANSPORT=streaming
# Default: 3000
PORT=3000

# Transport Type
# Options: stdio, streaming
# - stdio: For local CLI usage (Claude Code integration)
# - streaming: For HTTP server deployment
# Default: stdio
TRANSPORT=stdio

# Log Level
# Options: debug, info, warn, error
# Default: info
LOG_LEVEL=info

# ============================================
# Context Gathering Configuration
# ============================================

# Enable Serena MCP Integration
# Provides semantic code search and LSP features
# Default: true
ENABLE_SERENA=true

# Enable Graph Memory MCP Integration
# Provides context persistence and project history
# Default: true
ENABLE_MEMORY=true

# Enable CCLSP MCP Integration
# Provides language server protocol features
# Default: true
ENABLE_CCLSP=true

# Maximum Context Tokens
# Controls how much context is gathered from external sources
# Default: 32000
MAX_CONTEXT_TOKENS=32000

# Include File Content in Context
# When enabled, includes actual file contents (not just references)
# Default: true
INCLUDE_FILE_CONTENT=true

# Include Git History in Context
# When enabled, includes recent git commit history
# Default: false
INCLUDE_GIT_HISTORY=false

# ============================================
# Production Deployment (Optional)
# ============================================

# Node Environment
# Options: development, production, test
# Default: development
NODE_ENV=development

# Enable API Documentation (Swagger)
# Set to 'false' in production for security
# Default: true
ENABLE_API_DOCS=true

# CORS Origin (for HTTP transport)
# Comma-separated list of allowed origins
# Default: http://localhost:3000
CORS_ORIGIN=http://localhost:3000

# Rate Limiting (for HTTP transport)
# Requests allowed per window
# Default: 100 requests per 15 minutes
RATE_LIMIT_MAX_REQUESTS=100
RATE_LIMIT_WINDOW_MS=900000
