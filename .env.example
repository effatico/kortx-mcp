# OpenAI Configuration
# Required: Your OpenAI API key for GPT-5 access
OPENAI_API_KEY=your_openai_api_key_here

# Optional: GPT-5 model to use (default: gpt-5-mini)
# Options: gpt-5, gpt-5-mini, gpt-5-nano, gpt-5-codex
# - gpt-5: Best for complex reasoning, broad world knowledge, and multi-step agentic tasks
# - gpt-5-mini: Cost-optimized reasoning and chat; balances speed, cost, and capability (default)
# - gpt-5-nano: High-throughput tasks, especially simple instruction-following or classification
# - gpt-5-codex: Optimized for code generation, refactoring, debugging, and code explanations
OPENAI_MODEL=gpt-5-mini

# Optional: Reasoning effort level (default: medium)
# Options: minimal, low, medium, high
# - minimal: Very few reasoning tokens, fastest time-to-first-token
# - low: Favors speed and fewer tokens
# - medium: Balanced reasoning and speed (default)
# - high: More thorough reasoning, best for complex tasks
OPENAI_REASONING_EFFORT=medium

# Optional: Output verbosity level (default: medium)
# Options: low, medium, high
# - low: Concise answers, shorter code with minimal commentary
# - medium: Balanced output length
# - high: Thorough explanations, longer structured code with inline explanations
OPENAI_VERBOSITY=medium

# Optional: Maximum tokens for output (default: 4096)
OPENAI_MAX_TOKENS=4096

# Server Configuration
# Optional: Server name (default: kortx-mcp)
SERVER_NAME=kortx-mcp

# Optional: Server version (default: 0.1.0)
SERVER_VERSION=0.1.0

# Optional: Port for HTTP transport (default: 3000)
# Note: Currently only stdio transport is implemented
PORT=3000

# Optional: Transport mode (default: stdio)
# Options: stdio, streaming
TRANSPORT=stdio

# Logging Configuration
# Optional: Log level for structured logging (default: info)
# Options: debug, info, warn, error
# - debug: Detailed information for debugging (includes LLM requests, context gathering)
# - info: General informational messages (tool calls, responses, lifecycle events)
# - warn: Warning messages for potentially problematic situations
# - error: Error messages for failures
LOG_LEVEL=info

# Optional: Node environment (affects log formatting)
# - development: Pretty-printed, colorized logs with timestamps
# - production: JSON-formatted logs for log aggregation systems
NODE_ENV=development

# Context Gathering Configuration
# Optional: Enable Serena MCP integration (default: true)
ENABLE_SERENA=true

# Optional: Enable Memory MCP integration (default: true)
ENABLE_MEMORY=true

# Optional: Enable CCLSP MCP integration (default: true)
ENABLE_CCLSP=true

# Optional: Maximum tokens for context gathering (default: 32000)
MAX_CONTEXT_TOKENS=32000

# Optional: Include file content in context (default: true)
INCLUDE_FILE_CONTENT=true

# Optional: Include git history in context (default: false)
INCLUDE_GIT_HISTORY=false

# Security Configuration
# Optional: Enable rate limiting (default: true)
ENABLE_RATE_LIMITING=true

# Optional: Maximum requests per hour per client (default: 100)
MAX_REQUESTS_PER_HOUR=100

# Optional: Maximum tokens per single request (default: 50000)
MAX_TOKENS_PER_REQUEST=50000

# Optional: Maximum tokens per hour per client (default: 500000)
MAX_TOKENS_PER_HOUR=500000

# Optional: Request timeout in milliseconds (default: 60000)
REQUEST_TIMEOUT_MS=60000

# Optional: Maximum input size in bytes (default: 100000 - 100KB)
MAX_INPUT_SIZE=100000

# Perplexity Configuration
# Required: Your Perplexity API key for Sonar model access
PERPLEXITY_API_KEY=your_perplexity_api_key_here

# Optional: Perplexity Sonar model to use (default: sonar)
# Options: sonar, sonar-pro, sonar-deep-research, sonar-reasoning, sonar-reasoning-pro
# - sonar: Lightweight search model with fast response times
# - sonar-pro: Advanced search with enhanced accuracy and comprehensive results
# - sonar-deep-research: Exhaustive research with in-depth analysis and extensive sourcing
# - sonar-reasoning: Fast reasoning model optimized for analytical tasks
# - sonar-reasoning-pro: Premier reasoning model for complex analytical challenges (default)
PERPLEXITY_MODEL=sonar

# Optional: Temperature for response generation (default: 0.2)
# Range: 0-2. Lower values make output more focused and deterministic
PERPLEXITY_TEMPERATURE=0.2

# Optional: Maximum tokens for output (default: 4096)
PERPLEXITY_MAX_TOKENS=4096

# Optional: Search mode (default: web)
# Options: academic, sec, web
# - academic: Search academic papers and scholarly content
# - sec: Search SEC filings and financial documents
# - web: General web search across all domains
PERPLEXITY_SEARCH_MODE=web

# Optional: Return images in search results (default: false)
PERPLEXITY_RETURN_IMAGES=false

# Optional: Return related questions for follow-up (default: false)
PERPLEXITY_RETURN_RELATED_QUESTIONS=false
